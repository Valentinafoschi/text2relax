{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPK+ZYITZh1o3bq6/S9Ujjg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valentinafoschi/text2relax/blob/main/rumori_bianchi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "questo progetto ha come obiettivo quello di generare un audio + video rilassanti a partire da una descrizione testuale.\n",
        "\n",
        "testo --> paesaggi sonori + visivi"
      ],
      "metadata": {
        "id": "FX3LxX-J9Dtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet numpy==1.24.4 scipy==1.11.1\n"
      ],
      "metadata": {
        "id": "dYmoSDbkDYPR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet librosa==0.10.1 soundfile==0.12.1 moviepy==1.0.3\n"
      ],
      "metadata": {
        "id": "v2ePmHGmDq_9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 \\\n",
        "  open-clip-torch==2.24.0\n"
      ],
      "metadata": {
        "id": "6dAcWkKRD4V5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "facciamo un check per vedere se funziona tutto (da eliminare in fase di consegna del progetto)"
      ],
      "metadata": {
        "id": "mqafhoB2CMrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy, scipy, librosa, torch, moviepy, soundfile\n",
        "print(\"NumPy\", numpy.__version__)\n",
        "print(\"SciPy\", scipy.__version__)\n",
        "print(\"librosa\", librosa.__version__)\n",
        "print(\"PyTorch\", torch.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy4TeVJZCRGa",
        "outputId": "adc50989-f465-4d5f-d414-cfd6db28d3cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy 1.24.4\n",
            "SciPy 1.11.1\n",
            "librosa 0.10.1\n",
            "PyTorch 2.2.2+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "di seguito importiamo tutte le librerie necessarie per il progetto\n"
      ],
      "metadata": {
        "id": "LwSKDtcmGT8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, random, math, colorsys, re\n",
        "from typing import Dict, Any, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import librosa, soundfile as sf\n",
        "from scipy.signal import butter, lfilter, fftconvolve, welch\n",
        "from moviepy.editor import VideoClip, AudioFileClip\n",
        "\n",
        "import open_clip\n",
        "\n",
        "# CREAZIONE CARTELLE BASE PROGETTO\n",
        "for d in [\"data\",\"results\",\"checkpoints\"]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# seed + device\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "2bvLI1CLFWVV",
        "outputId": "812636e3-5980-47f2-eef1-a6ad7318b0fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "scegliamo le caratteristiche visive che il nostro modello deve generare e i tipi di audio che dovrà poi prevedere.\n",
        "pattern scelti:\n",
        "- **electric** --> lampi o scariche che attraversano lo schermo\n",
        "***snow** ---> fiocchi di neve che cadono\n",
        "***waves** --> onde sinusoidali lente\n",
        "***ripple** --> onde circolari concentriche tipo goccia in acqua\n",
        "***rotating shapes** --> cerchi, quadrati o triangoli che ruotano lentamente\n",
        "\n"
      ],
      "metadata": {
        "id": "CXzs5cweGZSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NOISE_TYPES = [\"white\", \"pink\", \"brown\", \"blue\"] #audio\n",
        "PATTERNS = [\"electric\", \"snow\", \"waves\", \"ripple\", \"rotatingShapes\"] #video\n",
        "\n",
        "RANGES = {\n",
        "    # AUDIO\n",
        "    \"duration_sec\": (30, 90),\n",
        "    \"lp_cutoff\": (300, 8000),   # filtro low-pass\n",
        "    \"hp_cutoff\": (20, 2000),    # filtro high-pass\n",
        "    \"reverb_mix\": (0.0, 0.6),   # quantità di riverbero\n",
        "    \"fade_in_sec\": (0.0, 5.0),  # durata del fade-in\n",
        "    \"fade_out_sec\": (0.0, 5.0), # durata del fade-out\n",
        "    \"rain_level\": (0.0, 1.0),   # intensità del pioggia\n",
        "    \"waves_level\": (0.0, 1.0),  # intensità delle onde\n",
        "    \"ripple_level\": (0.0, 1.0), # intensità delle goccie\n",
        "\n",
        "    # VIDEO\n",
        "    \"hue_main\": (0.0, 1.0),     # tonalità colore HSV\n",
        "    \"sat\": (0.2, 1.0),          # saturazione colore HSV\n",
        "    \"value\": (0.2, 1.0),        # luminosità colore HSV\n",
        "    \"motion_speed\": (0.05, 0.6),    # velocità movimento\n",
        "    \"motion_amplitude\": (0.1, 1.0), # ampiezza movimento\n",
        "    \"scene_brightness\": (0.2, 1.0), # luminosità globale\n",
        "    \"sync_to_audio\": (0.0, 1.0),    # sincronizzazione con audio\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "# funzione per normalizzare un parametro\n",
        "def norm_param (name, x):\n",
        "  lo, hi = RANGES[name]\n",
        "  return (x - lo) / (hi - lo + 1e-9)\n",
        "\n",
        "\n",
        "# funzione per denormalizzare un parametro\n",
        "def denorm_param (name, x01):\n",
        "  lo, hi = RANGES[name]\n",
        "  return float(lo + x01 * (hi - lo))"
      ],
      "metadata": {
        "id": "qk7OTXpoJAjQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nella cella seguente ci occupiamo del renderer audio, cioè la parte che a partire dai parametri audio previsti dal modello genera il file .wav vero e proprio."
      ],
      "metadata": {
        "id": "sHPJx6-VOGgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SR = 22050 # quante vote viene campionato il segnale audio\n",
        "\n",
        "# filtraggio frequenza\n",
        "def _butter_filter (x, cutoff, btype):\n",
        "  if cutoff <= 0:\n",
        "    return x\n",
        "  nyq = 0.5 * SR\n",
        "  cutoff = min(cutoff, nyq-100)\n",
        "  b, a = butter(4, cutoff / nyq, btype=btype)\n",
        "  return lfilter(b, a, x)\n",
        "\n",
        "# generazione rumore base\n",
        "def _gen_noise (kind, n):\n",
        "  if kind == \"white\": #frequenze uguali\n",
        "    x = np.random.randn(n)\n",
        "  elif kind==\"pink\": # più energia sulle basse frequenze\n",
        "    x = _butter_filter(np.random.randn(n), 500, 'low')\n",
        "  elif kind==\"brown\": # ancora più basse frequenze\n",
        "    x = np.cumsum(np.random.randn(n)); x /= (np.abs(x).max()+1e-9)\n",
        "  elif kind==\"blue\": # frequenze più alte\n",
        "    x = _butter_filter(np.random.randn(n), 3000, 'high')\n",
        "  return x.astype(np.float32)\n",
        "\n",
        "# genera un'onda alla frequenza\n",
        "def _sine(freq, n): t = np.arange(n)/SR; return np.sin(2*np.pi*freq*t).astype(np.float32)\n",
        "\n",
        "#simula le gocce di pioggia\n",
        "def _rain(n, level):\n",
        "    if level<=1e-3: return np.zeros(n, np.float32)\n",
        "    y = np.zeros(n, np.float32); drops = int(level*3*n/SR)\n",
        "    for _ in range(drops):\n",
        "        i = np.random.randint(0, n-200)\n",
        "        y[i:i+200] += (np.hanning(200)*np.random.uniform(0.3,1.0)).astype(np.float32)\n",
        "    return _butter_filter(y, 8000, 'low')\n",
        "\n",
        "# genera le onde del mare\n",
        "def _waves(n, level):\n",
        "    if level<=1e-3: return np.zeros(n, np.float32)\n",
        "    base = _sine(np.random.uniform(0.1,0.25), n)*0.3\n",
        "    mod  = _sine(np.random.uniform(0.01,0.05), n)*0.7 + 0.7\n",
        "    return (base*mod*level).astype(np.float32)\n",
        "\n",
        "\n",
        "\n",
        "# FUNZIONE PRINCIPALE DI RENDERING AUDIO\n",
        "\n",
        "def render_audio(params: Dict[str, Any], out_wav: str):\n",
        "    n = int(params[\"duration_sec\"]*SR)\n",
        "    x = _gen_noise(params[\"noise_type\"], n)\n",
        "    if params[\"lp_cutoff\"]>0: x = _butter_filter(x, params[\"lp_cutoff\"], 'low')\n",
        "    if params[\"hp_cutoff\"]>0: x = _butter_filter(x, params[\"hp_cutoff\"], 'high')\n",
        "    x = 0.7*x + 0.2*_rain(n, params[\"rain_level\"]) + 0.3*_waves(n, params[\"waves_level\"])\n",
        "    if params[\"reverb_mix\"]>0:\n",
        "        ir = np.exp(-np.linspace(0,1.0,int(0.12*SR))).astype(np.float32)\n",
        "        wet = fftconvolve(x, ir)[:n]\n",
        "        x = (1-params[\"reverb_mix\"])*x + params[\"reverb_mix\"]*wet\n",
        "    fi = int(params[\"fade_in_sec\"]*SR); fo = int(params[\"fade_out_sec\"]*SR)\n",
        "    if fi>0: x[:fi] *= np.linspace(0,1,fi)\n",
        "    if fo>0: x[-fo:] *= np.linspace(1,0,fo)\n",
        "    x /= (np.abs(x).max()+1e-9)\n",
        "    sf.write(out_wav, x, SR); return out_wav"
      ],
      "metadata": {
        "id": "4IQwq-MENtey"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ora creiamo l'animazione in base ai parametri e la sincronizziamo con l'audio"
      ],
      "metadata": {
        "id": "2ytpcZmlo-xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoClip\n",
        "import numpy as np, colorsys\n",
        "\n",
        "def _hsv_to_rgb01_scalar(h, s, v):\n",
        "    r,g,b = colorsys.hsv_to_rgb(h % 1.0, np.clip(s,0,1), np.clip(v,0,1))\n",
        "    return np.array([r, g, b], dtype=np.float32)\n",
        "\n",
        "def render_video_safe(params: dict, out_mp4: str, fps=10, size=(320,180)):\n",
        "    # Parametri\n",
        "    duration = float(params.get(\"duration_sec\", 5))\n",
        "    hue  = float(params.get(\"hue_main\", 0.55))\n",
        "    sat  = float(params.get(\"sat\", 0.9))\n",
        "    val0 = float(params.get(\"value\", 0.7))\n",
        "    speed = float(params.get(\"motion_speed\", 0.12))\n",
        "    amp   = float(params.get(\"motion_amplitude\", 0.5))\n",
        "    pattern = params.get(\"pattern\", \"waves\")\n",
        "\n",
        "    # Tinta base (V=1). La luminanza la mettiamo con z.\n",
        "    base_rgb = _hsv_to_rgb01_scalar(hue, sat, 1.0)\n",
        "\n",
        "    # Griglie una volta sola\n",
        "    w, h = int(size[0]), int(size[1])\n",
        "    X = np.linspace(0,1,w, dtype=np.float32)\n",
        "    Y = np.linspace(0,1,h, dtype=np.float32)\n",
        "    XX, YY = np.meshgrid(X, Y)\n",
        "    k = amp * 3.0\n",
        "\n",
        "    # Buffer riusato per il frame\n",
        "    frame = np.empty((h, w, 3), dtype=np.uint8)\n",
        "\n",
        "    def make_frame(t):\n",
        "        phase = 2*np.pi*(t*speed)\n",
        "\n",
        "        # z = mappa di luminanza in [0,1]\n",
        "        if pattern == \"waves\":\n",
        "            z = 0.5 + 0.5*np.sin(2*np.pi*(k*XX + 0.7*YY) + phase)\n",
        "\n",
        "        elif pattern == \"ripple\":\n",
        "            r = np.sqrt((XX-0.5)**2 + (YY-0.5)**2)\n",
        "            z = 0.5 + 0.5*np.sin(30*r - phase*5)\n",
        "\n",
        "        elif pattern == \"electric\":\n",
        "            z = 0.5 + 0.5*np.sin(16*XX + 12*YY + phase*6)\n",
        "\n",
        "        elif pattern == \"snow\":\n",
        "            z = np.full((h, w), val0, dtype=np.float32)\n",
        "            rng = np.random.default_rng(int(t*40))\n",
        "            for _ in range(40):\n",
        "                x = int(rng.integers(0, w)); y = int(rng.integers(0, h))\n",
        "                y0,y1 = max(0,y-1), min(h,y+1)\n",
        "                x0,x1 = max(0,x-1), min(w,x+1)\n",
        "                z[y0:y1, x0:x1] = 1.0\n",
        "\n",
        "        elif pattern == \"rotatingShapes\":\n",
        "            z = np.full((h, w), val0, dtype=np.float32)\n",
        "            cx, cy = w//2, h//2\n",
        "            rad = int(min(w,h)/5)\n",
        "            for i in range(5):\n",
        "                ang = phase + i*2*np.pi/5\n",
        "                x = int(cx + rad*np.cos(ang)); y = int(cy + rad*np.sin(ang))\n",
        "                y0,y1 = max(0,y-2), min(h,y+3)\n",
        "                x0,x1 = max(0,x-2), min(w,x+3)\n",
        "                z[y0:y1, x0:x1] = 1.0\n",
        "\n",
        "        else:\n",
        "            z = np.full((h, w), 0.5, dtype=np.float32)\n",
        "\n",
        "        # Applica luminanza e tinta\n",
        "        z = np.clip(z * val0, 0.0, 1.0).astype(np.float32)\n",
        "        frame[..., 0] = (z * (base_rgb[0] * 255.0)).astype(np.uint8)\n",
        "        frame[..., 1] = (z * (base_rgb[1] * 255.0)).astype(np.uint8)\n",
        "        frame[..., 2] = (z * (base_rgb[2] * 255.0)).astype(np.uint8)\n",
        "        return frame\n",
        "\n",
        "    clip = VideoClip(make_frame, duration=duration)\n",
        "    clip.write_videofile(out_mp4, fps=fps, codec=\"libx264\", audio=False,\n",
        "                         preset=\"ultrafast\", threads=1, verbose=True)\n",
        "    return out_mp4"
      ],
      "metadata": {
        "id": "UKu83BP4yF6H"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "visto che mi sta saltando la ram, proviamo prima a fare la cella per la generazione dell'audio, poi si vede"
      ],
      "metadata": {
        "id": "2ssEqJXrAMhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_audio_only = {\n",
        "    \"noise_type\":\"brown\", \"duration_sec\":6, \"lp_cutoff\":2500, \"hp_cutoff\":60,\n",
        "    \"reverb_mix\":0.15, \"fade_in_sec\":0.5, \"fade_out_sec\":0.5,\n",
        "    \"rain_level\":0.0, \"waves_level\":0.0,\n",
        "}\n",
        "_ = render_audio(demo_audio_only, \"results/_audio_only.wav\")\n",
        "print(\"OK audio:\", \"results/_audio_only.wav\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxmP2jLiAZOU",
        "outputId": "f5f44356-88ba-41d9-abe4-ec54e0cfba11"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK audio: results/_audio_only.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ora ci occupiamo della generazione del video muto\n",
        "\n"
      ],
      "metadata": {
        "id": "oyH1CmeyAv4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_vis = {\n",
        "    \"pattern\":\"waves\", \"duration_sec\":6,\n",
        "    \"hue_main\":0.55, \"sat\":0.9, \"value\":0.7,\n",
        "    \"motion_speed\":0.12, \"motion_amplitude\":0.5,\n",
        "}\n",
        "render_video_safe(params_vis, \"results/_video_only.mp4\", fps=10, size=(320,180))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "opz7fbkVPPUe",
        "outputId": "603fef7e-414e-445d-f266-67e736bc5178"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video results/_video_only.mp4.\n",
            "Moviepy - Writing video results/_video_only.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready results/_video_only.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'results/_video_only.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SONO ARRIVATA QUI E FINO A QUI RUNNA TUTTO"
      ],
      "metadata": {
        "id": "au1MHstbSMQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "visto che sincronizzare video e audio mi sfarfalla e mi fa uscire dalla ram visto che è molto pesante, per adesso la soluzione sara quella di creare video e audio indipendentmente (fatto gia nelle celle sopra) e poi di unirle con la funzione merge_audio_video. l'unica cosa sincronizzata sarà la durata del video e la durata dell'audio."
      ],
      "metadata": {
        "id": "uVDxqgooINon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "per evitare il crash aggiungiamo questa cosa"
      ],
      "metadata": {
        "id": "U-oVfecVKLE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -y -i results/_video_only_safe.mp4 -i results/_audio_only.wav \\\n",
        "  -c:v copy -c:a aac -b:a 192k -shortest results/_final_av.mp4\n"
      ],
      "metadata": {
        "id": "VUgWu2P4KZIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoFileClip, AudioFileClip\n",
        "\n",
        "def merge_audio_video(video_path: str, audio_path: str, out_path: str,\n",
        "                      codec=\"libx264\", audio_codec=\"aac\", preset=\"ultrafast\", threads=1):\n",
        "    v = VideoFileClip(video_path)\n",
        "    a = AudioFileClip(audio_path)\n",
        "\n",
        "    # allinea le durate → taglia audio se più lungo\n",
        "    a = a.subclip(0, v.duration)\n",
        "\n",
        "    final = v.set_audio(a)\n",
        "    final.write_videofile(out_path, fps=v.fps,\n",
        "                          codec=codec, audio_codec=audio_codec,\n",
        "                          preset=preset, threads=threads, verbose=True)\n",
        "\n",
        "    # chiudi risorse\n",
        "    a.close(); v.close(); final.close()\n",
        "    return out_path"
      ],
      "metadata": {
        "id": "mNv-UmVaInIW"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}